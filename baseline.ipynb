{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6228bc1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T16:23:18.746936Z",
     "iopub.status.busy": "2025-05-21T16:23:18.746641Z",
     "iopub.status.idle": "2025-05-21T16:23:24.360050Z",
     "shell.execute_reply": "2025-05-21T16:23:24.359191Z"
    },
    "id": "xSkgt1zf-raF",
    "outputId": "59f4a52f-5eb4-41e5-9fba-07432989fe78",
    "papermill": {
     "duration": 5.620104,
     "end_time": "2025-05-21T16:23:24.361690",
     "exception": false,
     "start_time": "2025-05-21T16:23:18.741586",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/gellert/Documents/UNI/DeepLearning/hackaton/.venv/bin/pip\n"
     ]
    }
   ],
   "source": [
    "!which pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c86aceda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T16:23:24.371427Z",
     "iopub.status.busy": "2025-05-21T16:23:24.371136Z",
     "iopub.status.idle": "2025-05-21T16:23:28.413941Z",
     "shell.execute_reply": "2025-05-21T16:23:28.413071Z"
    },
    "id": "5oR2D2Us-xSQ",
    "outputId": "7086cadf-a7fe-4d75-f271-6339bee8164d",
    "papermill": {
     "duration": 4.049235,
     "end_time": "2025-05-21T16:23:28.415556",
     "exception": false,
     "start_time": "2025-05-21T16:23:24.366321",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'hackaton'...\n",
      "remote: Enumerating objects: 81, done.\u001b[K\n",
      "remote: Counting objects: 100% (19/19), done.\u001b[K\n",
      "remote: Compressing objects: 100% (19/19), done.\u001b[K\n",
      "remote: Total 81 (delta 7), reused 0 (delta 0), pack-reused 62 (from 2)\u001b[K\n",
      "Receiving objects: 100% (81/81), 105.83 MiB | 2.57 MiB/s, done.\n",
      "Resolving deltas: 100% (8/8), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone --branch baselineCe https://github.com/Graph-Classification-Noisy-Label/hackaton.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c129091c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T16:23:28.427121Z",
     "iopub.status.busy": "2025-05-21T16:23:28.426849Z",
     "iopub.status.idle": "2025-05-21T16:23:28.432903Z",
     "shell.execute_reply": "2025-05-21T16:23:28.432144Z"
    },
    "id": "tEhfPly6-7UK",
    "outputId": "3078ee06-6312-4fca-f5f9-888fa628c80a",
    "papermill": {
     "duration": 0.013251,
     "end_time": "2025-05-21T16:23:28.434119",
     "exception": false,
     "start_time": "2025-05-21T16:23:28.420868",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'hackaton/'\n",
      "/home/gellert/Documents/UNI/DeepLearning/hackaton\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gellert/Documents/UNI/DeepLearning/hackaton/.venv/lib/python3.10/site-packages/IPython/core/magics/osm.py:393: UserWarning: This is now an optional IPython functionality, using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n"
     ]
    }
   ],
   "source": [
    "%cd hackaton/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d48103c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T16:23:28.444590Z",
     "iopub.status.busy": "2025-05-21T16:23:28.444373Z",
     "iopub.status.idle": "2025-05-21T16:24:52.394975Z",
     "shell.execute_reply": "2025-05-21T16:24:52.393870Z"
    },
    "id": "PxBvwB0_6xI8",
    "outputId": "5933387c-2cfb-474f-d842-f36a3e2d2a73",
    "papermill": {
     "duration": 83.957666,
     "end_time": "2025-05-21T16:24:52.396720",
     "exception": false,
     "start_time": "2025-05-21T16:23:28.439054",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: gdown: command not found\n"
     ]
    }
   ],
   "source": [
    "!gdown --folder https://drive.google.com/drive/folders/1Z-1JkPJ6q4C6jX4brvq1VRbJH5RPUCAk -O datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80dab1bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T16:24:52.425017Z",
     "iopub.status.busy": "2025-05-21T16:24:52.424626Z",
     "iopub.status.idle": "2025-05-21T16:24:52.547748Z",
     "shell.execute_reply": "2025-05-21T16:24:52.546915Z"
    },
    "id": "1rockhiQ7Nny",
    "outputId": "2cd2e6f4-5f8f-4a62-f0ec-53e6fc78c9b7",
    "papermill": {
     "duration": 0.138969,
     "end_time": "2025-05-21T16:24:52.549260",
     "exception": false,
     "start_time": "2025-05-21T16:24:52.410291",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16K\n",
      "drwxrwxr-x 2 gellert gellert 4.0K May 23 15:37 A\n",
      "drwxrwxr-x 2 gellert gellert 4.0K May 23 15:40 B\n",
      "drwxrwxr-x 2 gellert gellert 4.0K May 24 13:58 C\n",
      "drwxrwxr-x 2 gellert gellert 4.0K May 23 15:26 D\n"
     ]
    }
   ],
   "source": [
    "!ls -lh datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "817b1078",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T16:24:52.573213Z",
     "iopub.status.busy": "2025-05-21T16:24:52.572983Z",
     "iopub.status.idle": "2025-05-21T16:25:02.508892Z",
     "shell.execute_reply": "2025-05-21T16:25:02.508145Z"
    },
    "id": "lAQuCuIoBbq5",
    "papermill": {
     "duration": 9.949638,
     "end_time": "2025-05-21T16:25:02.510764",
     "exception": false,
     "start_time": "2025-05-21T16:24:52.561126",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "# Load utility functions from cloned repository\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory of src to the system path\n",
    "sys.path.append(os.path.abspath('./'))\n",
    "\n",
    "from src.loadData import GraphDataset\n",
    "from src.utils import set_seed\n",
    "from src.models import GNN\n",
    "import argparse\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "# Set the random seed\n",
    "set_seed()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0a9c70d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T16:25:02.539544Z",
     "iopub.status.busy": "2025-05-21T16:25:02.539080Z",
     "iopub.status.idle": "2025-05-21T16:25:02.543151Z",
     "shell.execute_reply": "2025-05-21T16:25:02.542244Z"
    },
    "id": "Dyf0I2-t9IcW",
    "papermill": {
     "duration": 0.019268,
     "end_time": "2025-05-21T16:25:02.544583",
     "exception": false,
     "start_time": "2025-05-21T16:25:02.525315",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_zeros(data):\n",
    "    data.x = torch.zeros(data.num_nodes, dtype=torch.long)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3622cfa1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T16:25:02.571253Z",
     "iopub.status.busy": "2025-05-21T16:25:02.570992Z",
     "iopub.status.idle": "2025-05-21T16:25:02.576383Z",
     "shell.execute_reply": "2025-05-21T16:25:02.575520Z"
    },
    "id": "3jKvoQYI9Zbc",
    "papermill": {
     "duration": 0.019599,
     "end_time": "2025-05-21T16:25:02.577661",
     "exception": false,
     "start_time": "2025-05-21T16:25:02.558062",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(data_loader, model, optimizer, device, save_checkpoints, checkpoint_path, current_epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data in tqdm(data_loader, desc=\"Iterating training graphs\", unit=\"batch\"):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        node_emb, output = model(data)\n",
    "        loss = model.loss(node_emb, output, data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        pred = output.argmax(dim=1)\n",
    "        correct += (pred == data.y).sum().item()\n",
    "        total += data.y.size(0)\n",
    "\n",
    "    # Save checkpoints if required\n",
    "    if save_checkpoints:\n",
    "        checkpoint_file = f\"{checkpoint_path}_epoch_{current_epoch + 1}.pth\"\n",
    "        torch.save(model.state_dict(), checkpoint_file)\n",
    "        print(f\"Checkpoint saved at {checkpoint_file}\")\n",
    "\n",
    "    return total_loss / len(data_loader),  correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6139b912",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T16:25:02.601830Z",
     "iopub.status.busy": "2025-05-21T16:25:02.601604Z",
     "iopub.status.idle": "2025-05-21T16:25:02.606506Z",
     "shell.execute_reply": "2025-05-21T16:25:02.605692Z"
    },
    "id": "8peFiIS19ZpK",
    "papermill": {
     "duration": 0.017908,
     "end_time": "2025-05-21T16:25:02.607848",
     "exception": false,
     "start_time": "2025-05-21T16:25:02.589940",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def evaluate(data_loader, model, device, calculate_accuracy=False):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    predictions = []\n",
    "    total_loss = 0\n",
    "    true_labels = []\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(data_loader, desc=\"Iterating eval graphs\", unit=\"batch\"):\n",
    "            data = data.to(device)\n",
    "            node_emb, output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            predictions.extend(pred.cpu().numpy())\n",
    "            \n",
    "            if calculate_accuracy:\n",
    "                true_labels.extend(data.y.cpu().numpy())\n",
    "                correct += (pred == data.y).sum().item()\n",
    "                total += data.y.size(0)\n",
    "    if calculate_accuracy:\n",
    "        f1 = f1_score(true_labels, predictions, average='weighted')  \n",
    "        accuracy = correct / total\n",
    "        return  total_loss / len(data_loader), accuracy, f1\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fbdbd871",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T16:25:02.630783Z",
     "iopub.status.busy": "2025-05-21T16:25:02.630589Z",
     "iopub.status.idle": "2025-05-21T16:25:02.634504Z",
     "shell.execute_reply": "2025-05-21T16:25:02.633939Z"
    },
    "id": "WanuZKxy9Zs-",
    "papermill": {
     "duration": 0.016728,
     "end_time": "2025-05-21T16:25:02.635694",
     "exception": false,
     "start_time": "2025-05-21T16:25:02.618966",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_predictions(predictions, test_path):\n",
    "    script_dir = os.getcwd() \n",
    "    submission_folder = os.path.join(script_dir, \"submission\")\n",
    "    test_dir_name = os.path.basename(os.path.dirname(test_path))\n",
    "    \n",
    "    os.makedirs(submission_folder, exist_ok=True)\n",
    "    \n",
    "    output_csv_path = os.path.join(submission_folder, f\"testset_{test_dir_name}.csv\")\n",
    "    \n",
    "    test_graph_ids = list(range(len(predictions)))\n",
    "    output_df = pd.DataFrame({\n",
    "        \"id\": test_graph_ids,\n",
    "        \"pred\": predictions\n",
    "    })\n",
    "    \n",
    "    output_df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Predictions saved to {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fc3d24da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T16:25:02.659059Z",
     "iopub.status.busy": "2025-05-21T16:25:02.658861Z",
     "iopub.status.idle": "2025-05-21T16:25:02.663407Z",
     "shell.execute_reply": "2025-05-21T16:25:02.662776Z"
    },
    "id": "uyHIJS5U9ZzB",
    "papermill": {
     "duration": 0.017765,
     "end_time": "2025-05-21T16:25:02.664538",
     "exception": false,
     "start_time": "2025-05-21T16:25:02.646773",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_training_progress(train_losses, train_accuracies, output_dir):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot losscolabes, label=\"Training Loss\", color='blue')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss per Epoch')\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_accuracies, label=\"Training Accuracy\", color='green')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training Accuracy per Epoch')\n",
    "\n",
    "    # Save plots in the current directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"training_progress.png\"))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "22574fa5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T16:25:02.687544Z",
     "iopub.status.busy": "2025-05-21T16:25:02.687339Z",
     "iopub.status.idle": "2025-05-21T16:25:02.690976Z",
     "shell.execute_reply": "2025-05-21T16:25:02.690429Z"
    },
    "papermill": {
     "duration": 0.016577,
     "end_time": "2025-05-21T16:25:02.692205",
     "exception": false,
     "start_time": "2025-05-21T16:25:02.675628",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_user_input(prompt, default=None, required=False, type_cast=str):\n",
    "\n",
    "    while True:\n",
    "        user_input = input(f\"{prompt} [{default}]: \")\n",
    "        \n",
    "        if user_input == \"\" and required:\n",
    "            print(\"This field is required. Please enter a value.\")\n",
    "            continue\n",
    "        \n",
    "        if user_input == \"\" and default is not None:\n",
    "            return default\n",
    "        \n",
    "        if user_input == \"\" and not required:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            return type_cast(user_input)\n",
    "        except ValueError:\n",
    "            print(f\"Invalid input. Please enter a valid {type_cast.__name__}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "139e88b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T16:25:02.715440Z",
     "iopub.status.busy": "2025-05-21T16:25:02.715193Z",
     "iopub.status.idle": "2025-05-21T16:25:02.720119Z",
     "shell.execute_reply": "2025-05-21T16:25:02.719542Z"
    },
    "papermill": {
     "duration": 0.017703,
     "end_time": "2025-05-21T16:25:02.721184",
     "exception": false,
     "start_time": "2025-05-21T16:25:02.703481",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_arguments():\n",
    "    args = {}\n",
    "    # args['train_path'] = get_user_input(\"Path to the training dataset (optional)\")\n",
    "    args['train_path'] = \"./datasets/A/train.json.gz\"\n",
    "\n",
    "    # args['test_path'] = get_user_input(\"Path to the test dataset\", required=True)\n",
    "    args['test_path'] = \"./datasets/A/test.json.gz\"\n",
    "    \n",
    "    # args['num_checkpoints'] = get_user_input(\"Number of checkpoints to save during training\", type_cast=int)\n",
    "    args['num_checkpoints'] = 5\n",
    "    \n",
    "    # args['device'] = get_user_input(\"Which GPU to use if any\", default=1, type_cast=int)\n",
    "    args['device'] = 0\n",
    "    \n",
    "    # args['gnn'] = get_user_input(\"GNN type (gin, gin-virtual, gcn, gcn-virtual)\", default='gin')\n",
    "    args['gnn'] = \"gin-virtual\"\n",
    "    \n",
    "    # args['drop_ratio'] = get_user_input(\"Dropout ratio\", default=0.0, type_cast=float)\n",
    "    args['drop_ratio'] = 0.3\n",
    "    \n",
    "    # args['num_layer'] = get_user_input(\"Number of GNN message passing layers\", default=5, type_cast=int)\n",
    "    args['num_layer'] = 5\n",
    "    \n",
    "    # args['emb_dim'] = get_user_input(\"Dimensionality of hidden units in GNNs\", default=300, type_cast=int)\n",
    "    args['emb_dim'] = 64\n",
    "    \n",
    "    # args['batch_size'] = get_user_input(\"Input batch size for training\", default=32, type_cast=int)\n",
    "    args['batch_size'] = 32\n",
    "    \n",
    "    # args['epochs'] = get_user_input(\"Number of epochs to train\", default=10, type_cast=int)\n",
    "    args['epochs'] = 2\n",
    "    args[\"warmup_epochs\"] = 1\n",
    "    \n",
    "    # args['baseline_mode'] = get_user_input(\"Baseline mode: 1 (CE), 2 (Noisy CE)\", default=1, type_cast=int)\n",
    "    args['baseline_mode'] = 2\n",
    "    \n",
    "    # args['noise_prob'] = get_user_input(\"Noise probability p (used if baseline_mode=2)\", default=0.2, type_cast=float)\n",
    "    args['noise_prob'] = 0.2\n",
    "\n",
    "    args[\"num_voters\"] = 2\n",
    "\n",
    "\n",
    "    \n",
    "    return argparse.Namespace(**args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "45bffa19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T16:25:02.744653Z",
     "iopub.status.busy": "2025-05-21T16:25:02.744453Z",
     "iopub.status.idle": "2025-05-21T16:25:02.849698Z",
     "shell.execute_reply": "2025-05-21T16:25:02.848596Z"
    },
    "papermill": {
     "duration": 0.118164,
     "end_time": "2025-05-21T16:25:02.850799",
     "exception": true,
     "start_time": "2025-05-21T16:25:02.732635",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arguments received:\n",
      "train_path: ./datasets/A/train.json.gz\n",
      "test_path: ./datasets/A/test.json.gz\n",
      "num_checkpoints: 5\n",
      "device: 0\n",
      "gnn: gin-virtual\n",
      "drop_ratio: 0.3\n",
      "num_layer: 5\n",
      "emb_dim: 64\n",
      "batch_size: 32\n",
      "epochs: 2\n",
      "warmup_epochs: 1\n",
      "baseline_mode: 2\n",
      "noise_prob: 0.2\n",
      "num_voters: 2\n"
     ]
    }
   ],
   "source": [
    "def populate_args(args):\n",
    "    print(\"Arguments received:\")\n",
    "    for key, value in vars(args).items():\n",
    "        print(f\"{key}: {value}\")\n",
    "args = get_arguments()\n",
    "populate_args(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "09b57d62",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NoisyCrossEntropyLoss(torch.nn.Module):\n",
    "    def __init__(self, p_noisy):\n",
    "        super().__init__()\n",
    "        self.p = p_noisy\n",
    "        self.ce = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        losses = self.ce(logits, targets)\n",
    "        weights = (1 - self.p) + self.p * (1 - torch.nn.functional.one_hot(targets, num_classes=logits.size(1)).float().sum(dim=1))\n",
    "        return (losses * weights).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "1e1baeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.conv import GNN_node, GNN_node_Virtualnode\n",
    "from torch_geometric.nn import global_add_pool, global_mean_pool, global_max_pool, GlobalAttention, Set2Set\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_class, num_layer = 5, emb_dim = 300, latent_dim = 16,\n",
    "                    gnn_type = 'gin', virtual_node = True, residual = False, drop_ratio = 0.5, JK = \"last\", graph_pooling = \"mean\"):\n",
    "        '''\n",
    "            num_tasks (int): number of labels to be predicted\n",
    "            virtual_node (bool): whether to add virtual node or not\n",
    "        '''\n",
    "\n",
    "        super(GNN, self).__init__()\n",
    "\n",
    "        self.num_layer = num_layer\n",
    "        self.drop_ratio = drop_ratio\n",
    "        self.JK = JK\n",
    "        self.emb_dim = emb_dim\n",
    "        self.num_class = num_class\n",
    "        self.graph_pooling = graph_pooling\n",
    "\n",
    "        if self.num_layer < 2:\n",
    "            raise ValueError(\"Number of GNN layers must be greater than 1.\")\n",
    "\n",
    "        ### GNN to generate node embeddings\n",
    "        if virtual_node:\n",
    "            self.gnn_node = GNN_node_Virtualnode(num_layer, emb_dim, JK = JK, drop_ratio = drop_ratio, residual = residual, gnn_type = gnn_type)\n",
    "        else:\n",
    "            self.gnn_node = GNN_node(num_layer, emb_dim, JK = JK, drop_ratio = drop_ratio, residual = residual, gnn_type = gnn_type)\n",
    "\n",
    "\n",
    "        ### Pooling function to generate whole-graph embeddings\n",
    "        if self.graph_pooling == \"sum\":\n",
    "            self.pool = global_add_pool\n",
    "        elif self.graph_pooling == \"mean\":\n",
    "            self.pool = global_mean_pool\n",
    "        elif self.graph_pooling == \"max\":\n",
    "            self.pool = global_max_pool\n",
    "        elif self.graph_pooling == \"set2set\":\n",
    "            self.pool = Set2Set(emb_dim, processing_steps = 2)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid graph pooling type.\")\n",
    "        \n",
    "        self.node_emb = torch.nn.Linear(emb_dim, latent_dim)\n",
    "\n",
    "        self.edge_attr_decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(latent_dim * 2, latent_dim),\n",
    "            torch.nn.LeakyReLU(0.1),\n",
    "            torch.nn.Linear(latent_dim, 7)\n",
    "        )\n",
    "\n",
    "        self.classifier = torch.nn.Linear(emb_dim, 6)\n",
    "\n",
    "        self.criterion = NoisyCrossEntropyLoss(args.noise_prob)\n",
    "\n",
    "    def decode(self, z, edge_index):\n",
    "        adj_pred = torch.sigmoid(torch.mm(z, z.t()))\n",
    "        \n",
    "        edge_feat_input = torch.cat([z[edge_index[0]], z[edge_index[1]]], dim=1)\n",
    "        edge_attr_pred = torch.sigmoid(self.edge_attr_decoder(edge_feat_input))\n",
    "        \n",
    "        return adj_pred, edge_attr_pred\n",
    "\n",
    "    def forward(self, batched_data):\n",
    "        h_node = torch.nn.functional.leaky_relu(self.gnn_node(batched_data), 0.1)\n",
    "        h_graph = self.pool(h_node, batched_data.batch)\n",
    "\n",
    "        node_emb = self.node_emb(h_node)\n",
    "        return node_emb, self.classifier(h_graph)\n",
    "    \n",
    "    def recon_loss(self, z, data, alpha=0.1, beta=1, gamma=0.1):\n",
    "        adj_pred, edge_attr_pred = self.decode(z, data.edge_index)\n",
    "        adj_true = torch.zeros_like(adj_pred)\n",
    "        adj_true[data.edge_index[0], data.edge_index[1]]\n",
    "\n",
    "        adj_loss = F.binary_cross_entropy(adj_pred, adj_true)\n",
    "        edge_attr_loss = F.mse_loss(edge_attr_pred, data.edge_attr)\n",
    "\n",
    "        # kl_loss = -0.5 * torch.mean(torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1))\n",
    "        return alpha * adj_loss + beta * edge_attr_loss\n",
    "\n",
    "    def loss(self, node_emb, class_logits, data):\n",
    "        classification_loss = self.criterion(class_logits, data.y)\n",
    "        recon_loss = self.recon_loss(node_emb, data)\n",
    "\n",
    "        loss = 0.1 * recon_loss + classification_loss\n",
    "        return loss\n",
    "    \n",
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self, models: list, weights, device):\n",
    "        super().__init__()\n",
    "        self.models = nn.ModuleList(models)\n",
    "        # Trainable weights initialized equally\n",
    "        self.register_buffer(\"weights\", torch.tensor(weights, dtype=torch.float32))\n",
    "        self.weights = (self.weights / self.weights.sum(dim=0)).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = [model(x)[1] for model in self.models]\n",
    "        outputs = torch.stack(outputs)\n",
    "        weighted_avg = (outputs * self.weights.view(-1, 1, 1)).sum(dim=0)\n",
    "        return None, weighted_avg\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f5e9144f",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = GraphDataset(args.train_path, transform=add_zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5603ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_once(model, args, voter, full_dataset, test_dir_name, logs_folder, script_dir, device):\n",
    "    val_size = int(0.2 * len(full_dataset))\n",
    "    train_size = len(full_dataset) - val_size\n",
    "\n",
    "    generator = torch.Generator().manual_seed(voter+1)\n",
    "    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size], generator=generator)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "    checkpoint_path = os.path.join(script_dir, \"checkpoints\", f\"model_{test_dir_name}_voter_{voter}_best.pth\")\n",
    "    checkpoints_folder = os.path.join(script_dir, \"checkpoints\", test_dir_name)\n",
    "    os.makedirs(checkpoints_folder, exist_ok=True)\n",
    "\n",
    "    num_checkpoints = args.num_checkpoints if args.num_checkpoints else 3\n",
    "        \n",
    "\n",
    "    initial_lr = 1e-5\n",
    "    target_lr = 1e-3\n",
    "    minimum_lr = 1e-6\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=initial_lr)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=args.epochs-args.warmup_epochs, eta_min=minimum_lr)\n",
    "\n",
    "    num_epochs = args.epochs\n",
    "    best_f1 = 0.0   \n",
    "\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    if num_checkpoints > 1:\n",
    "        checkpoint_intervals = [int((i + 1) * num_epochs / num_checkpoints) for i in range(num_checkpoints)]\n",
    "    else:\n",
    "        checkpoint_intervals = [num_epochs]\n",
    "\n",
    "    def get_lr(epoch):\n",
    "        if epoch < args.warmup_epochs:\n",
    "            lr = initial_lr + (target_lr - initial_lr) * (epoch / args.warmup_epochs)\n",
    "        else:\n",
    "            scheduler.step()\n",
    "            lr = scheduler.get_last_lr()[-1]\n",
    "        return lr\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        lr = get_lr(epoch)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        print(f\"Current learning rate: {lr}\")\n",
    "\n",
    "        train_loss, train_acc = train(\n",
    "            train_loader, model, optimizer, device,\n",
    "            save_checkpoints=(epoch + 1 in checkpoint_intervals),\n",
    "            checkpoint_path=os.path.join(checkpoints_folder, f\"model_{test_dir_name}_voter_{voter}\"),\n",
    "            current_epoch=epoch\n",
    "        )\n",
    "\n",
    "        val_loss,val_acc, f1 = evaluate(val_loader, model, device, calculate_accuracy=True)\n",
    "\n",
    "        print(f\"Voter {voter}, Epoch {epoch + 1}/{num_epochs}, Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}, Val f1: {f1:.4f}\")\n",
    "        logging.info(f\"Voter {voter}, Epoch {epoch + 1}/{num_epochs}, Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}, Val f1: {f1:.4f}\")\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "        \n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(f\"Best model updated and saved at {checkpoint_path}\")\n",
    "    model.load_state_dict(torch.load(checkpoint_path))\n",
    "    plot_training_progress(train_losses, train_accuracies, os.path.join(logs_folder, f\"plots_voter_{voter}\"))\n",
    "    plot_training_progress(val_losses, val_accuracies, os.path.join(logs_folder, f\"plotsVal_voter_{voter}\"))\n",
    "\n",
    "    return model, best_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "92245277",
   "metadata": {},
   "outputs": [],
   "source": [
    "script_dir = os.getcwd() \n",
    "device = torch.device(f\"cuda:{args.device}\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "82e9e15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    if args.gnn == 'gin':\n",
    "        model = GNN(gnn_type='gin', num_class=6, num_layer=args.num_layer, emb_dim=args.emb_dim, drop_ratio=args.drop_ratio, virtual_node=False).to(device)\n",
    "    elif args.gnn == 'gin-virtual':\n",
    "        model = GNN(gnn_type='gin', num_class=6, num_layer=args.num_layer, emb_dim=args.emb_dim, drop_ratio=args.drop_ratio, virtual_node=True).to(device)\n",
    "    elif args.gnn == 'gcn':\n",
    "        model = GNN(gnn_type='gcn', num_class=6, num_layer=args.num_layer, emb_dim=args.emb_dim, drop_ratio=args.drop_ratio, virtual_node=False).to(device)\n",
    "    elif args.gnn == 'gcn-virtual':\n",
    "        model = GNN(gnn_type='gcn', num_class=6, num_layer=args.num_layer, emb_dim=args.emb_dim, drop_ratio=args.drop_ratio, virtual_node=True).to(device)\n",
    "    else:\n",
    "        raise ValueError('Invalid GNN type')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4a359c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current learning rate: 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs:   0%|          | 0/282 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 282/282 [00:32<00:00,  8.56batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /home/gellert/Documents/UNI/DeepLearning/hackaton/checkpoints/A/model_A_voter_0_epoch_1.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating eval graphs: 100%|██████████| 71/71 [00:06<00:00, 10.40batch/s]\n",
      "Voter 0, Epoch 1/2, Loss: 1.4850, Train Acc: 0.1730, Val Acc: 0.1760, Val f1: 0.0720\n",
      "Voter 0, Epoch 1/2, Loss: 1.4850, Train Acc: 0.1730, Val Acc: 0.1760, Val f1: 0.0720\n",
      "Voter 0, Epoch 1/2, Loss: 1.4850, Train Acc: 0.1730, Val Acc: 0.1760, Val f1: 0.0720\n",
      "Voter 0, Epoch 1/2, Loss: 1.4850, Train Acc: 0.1730, Val Acc: 0.1760, Val f1: 0.0720\n",
      "Voter 0, Epoch 1/2, Loss: 1.4850, Train Acc: 0.1730, Val Acc: 0.1760, Val f1: 0.0720\n",
      "Voter 0, Epoch 1/2, Loss: 1.4850, Train Acc: 0.1730, Val Acc: 0.1760, Val f1: 0.0720\n",
      "Voter 0, Epoch 1/2, Loss: 1.4850, Train Acc: 0.1730, Val Acc: 0.1760, Val f1: 0.0720\n",
      "Voter 0, Epoch 1/2, Loss: 1.4850, Train Acc: 0.1730, Val Acc: 0.1760, Val f1: 0.0720\n",
      "Voter 0, Epoch 1/2, Loss: 1.4850, Train Acc: 0.1730, Val Acc: 0.1760, Val f1: 0.0720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voter 0, Epoch 1/2, Loss: 1.4850, Train Acc: 0.1730, Val Acc: 0.1760, Val f1: 0.0720\n",
      "Best model updated and saved at /home/gellert/Documents/UNI/DeepLearning/hackaton/checkpoints/model_A_voter_0_best.pth\n",
      "Current learning rate: 1e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 282/282 [00:33<00:00,  8.45batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /home/gellert/Documents/UNI/DeepLearning/hackaton/checkpoints/A/model_A_voter_0_epoch_2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating eval graphs: 100%|██████████| 71/71 [00:06<00:00, 10.74batch/s]\n",
      "Voter 0, Epoch 2/2, Loss: 1.4772, Train Acc: 0.1779, Val Acc: 0.1777, Val f1: 0.0745\n",
      "Voter 0, Epoch 2/2, Loss: 1.4772, Train Acc: 0.1779, Val Acc: 0.1777, Val f1: 0.0745\n",
      "Voter 0, Epoch 2/2, Loss: 1.4772, Train Acc: 0.1779, Val Acc: 0.1777, Val f1: 0.0745\n",
      "Voter 0, Epoch 2/2, Loss: 1.4772, Train Acc: 0.1779, Val Acc: 0.1777, Val f1: 0.0745\n",
      "Voter 0, Epoch 2/2, Loss: 1.4772, Train Acc: 0.1779, Val Acc: 0.1777, Val f1: 0.0745\n",
      "Voter 0, Epoch 2/2, Loss: 1.4772, Train Acc: 0.1779, Val Acc: 0.1777, Val f1: 0.0745\n",
      "Voter 0, Epoch 2/2, Loss: 1.4772, Train Acc: 0.1779, Val Acc: 0.1777, Val f1: 0.0745\n",
      "Voter 0, Epoch 2/2, Loss: 1.4772, Train Acc: 0.1779, Val Acc: 0.1777, Val f1: 0.0745\n",
      "Voter 0, Epoch 2/2, Loss: 1.4772, Train Acc: 0.1779, Val Acc: 0.1777, Val f1: 0.0745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voter 0, Epoch 2/2, Loss: 1.4772, Train Acc: 0.1779, Val Acc: 0.1777, Val f1: 0.0745\n",
      "Best model updated and saved at /home/gellert/Documents/UNI/DeepLearning/hackaton/checkpoints/model_A_voter_0_best.pth\n",
      "Current learning rate: 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 282/282 [00:33<00:00,  8.29batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /home/gellert/Documents/UNI/DeepLearning/hackaton/checkpoints/A/model_A_voter_1_epoch_1.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating eval graphs: 100%|██████████| 71/71 [00:06<00:00, 10.41batch/s]\n",
      "Voter 1, Epoch 1/2, Loss: 1.4356, Train Acc: 0.2620, Val Acc: 0.2841, Val f1: 0.1565\n",
      "Voter 1, Epoch 1/2, Loss: 1.4356, Train Acc: 0.2620, Val Acc: 0.2841, Val f1: 0.1565\n",
      "Voter 1, Epoch 1/2, Loss: 1.4356, Train Acc: 0.2620, Val Acc: 0.2841, Val f1: 0.1565\n",
      "Voter 1, Epoch 1/2, Loss: 1.4356, Train Acc: 0.2620, Val Acc: 0.2841, Val f1: 0.1565\n",
      "Voter 1, Epoch 1/2, Loss: 1.4356, Train Acc: 0.2620, Val Acc: 0.2841, Val f1: 0.1565\n",
      "Voter 1, Epoch 1/2, Loss: 1.4356, Train Acc: 0.2620, Val Acc: 0.2841, Val f1: 0.1565\n",
      "Voter 1, Epoch 1/2, Loss: 1.4356, Train Acc: 0.2620, Val Acc: 0.2841, Val f1: 0.1565\n",
      "Voter 1, Epoch 1/2, Loss: 1.4356, Train Acc: 0.2620, Val Acc: 0.2841, Val f1: 0.1565\n",
      "Voter 1, Epoch 1/2, Loss: 1.4356, Train Acc: 0.2620, Val Acc: 0.2841, Val f1: 0.1565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voter 1, Epoch 1/2, Loss: 1.4356, Train Acc: 0.2620, Val Acc: 0.2841, Val f1: 0.1565\n",
      "Best model updated and saved at /home/gellert/Documents/UNI/DeepLearning/hackaton/checkpoints/model_A_voter_1_best.pth\n",
      "Current learning rate: 1e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 282/282 [00:33<00:00,  8.49batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /home/gellert/Documents/UNI/DeepLearning/hackaton/checkpoints/A/model_A_voter_1_epoch_2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating eval graphs: 100%|██████████| 71/71 [00:06<00:00, 11.23batch/s]\n",
      "Voter 1, Epoch 2/2, Loss: 1.4297, Train Acc: 0.2664, Val Acc: 0.2908, Val f1: 0.1489\n",
      "Voter 1, Epoch 2/2, Loss: 1.4297, Train Acc: 0.2664, Val Acc: 0.2908, Val f1: 0.1489\n",
      "Voter 1, Epoch 2/2, Loss: 1.4297, Train Acc: 0.2664, Val Acc: 0.2908, Val f1: 0.1489\n",
      "Voter 1, Epoch 2/2, Loss: 1.4297, Train Acc: 0.2664, Val Acc: 0.2908, Val f1: 0.1489\n",
      "Voter 1, Epoch 2/2, Loss: 1.4297, Train Acc: 0.2664, Val Acc: 0.2908, Val f1: 0.1489\n",
      "Voter 1, Epoch 2/2, Loss: 1.4297, Train Acc: 0.2664, Val Acc: 0.2908, Val f1: 0.1489\n",
      "Voter 1, Epoch 2/2, Loss: 1.4297, Train Acc: 0.2664, Val Acc: 0.2908, Val f1: 0.1489\n",
      "Voter 1, Epoch 2/2, Loss: 1.4297, Train Acc: 0.2664, Val Acc: 0.2908, Val f1: 0.1489\n",
      "Voter 1, Epoch 2/2, Loss: 1.4297, Train Acc: 0.2664, Val Acc: 0.2908, Val f1: 0.1489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voter 1, Epoch 2/2, Loss: 1.4297, Train Acc: 0.2664, Val Acc: 0.2908, Val f1: 0.1489\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "save() missing 1 required positional argument: 'f'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[98], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m     weights\u001b[38;5;241m.\u001b[39mappend(weight)\n\u001b[1;32m     21\u001b[0m ensemble \u001b[38;5;241m=\u001b[39m EnsembleModel(models, weights)\n\u001b[0;32m---> 23\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mensemble\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m, checkpoint_path\n",
      "\u001b[0;31mTypeError\u001b[0m: save() missing 1 required positional argument: 'f'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "test_dir_name = os.path.basename(os.path.dirname(args.test_path))\n",
    "logs_folder = os.path.join(script_dir, \"logs\", test_dir_name)\n",
    "log_file = os.path.join(logs_folder, f\"training.log\")\n",
    "os.makedirs(os.path.dirname(log_file), exist_ok=True)\n",
    "logging.basicConfig(filename=log_file, level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "logging.getLogger().addHandler(logging.StreamHandler())\n",
    "\n",
    "checkpoint_path = os.path.join(script_dir, \"checkpoints\", f\"model_{test_dir_name}_best.pth\")\n",
    "checkpoints_folder = os.path.join(script_dir, \"checkpoints\", test_dir_name)\n",
    "os.makedirs(checkpoints_folder, exist_ok=True)\n",
    "\n",
    "if args.train_path:\n",
    "\n",
    "    models, weights = [], []\n",
    "    for voter in range(args.num_voters):\n",
    "        model, weight = train_once(get_model(), args, voter, full_dataset, test_dir_name, logs_folder, script_dir, device)\n",
    "        models.append(model)\n",
    "        weights.append(weight)\n",
    "    ensemble = EnsembleModel(models, weights, device)\n",
    "\n",
    "    torch.save(ensemble.state_dict(), checkpoint_path)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19eb973",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ba668fff",
   "metadata": {
    "id": "xsXZIj4Mdu3I",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataset = GraphDataset(args.test_path, transform=add_zeros)\n",
    "test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "7828e983",
   "metadata": {
    "id": "x1OnGq_nCmTr",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating eval graphs: 100%|██████████| 74/74 [00:04<00:00, 15.13batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to /home/gellert/Documents/UNI/DeepLearning/hackaton/submission/testset_A.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = EnsembleModel([get_model() for _ in range(args.num_voters)], [1 for _ in range(args.num_voters)], device)\n",
    "model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "predictions = evaluate(test_loader, model, device, calculate_accuracy=False)\n",
    "save_predictions(predictions, args.test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343c36b2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 108.457758,
   "end_time": "2025-05-21T16:25:04.482169",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-21T16:23:16.024411",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
